# AlphaGo-Lite â˜… Reinforcement Learning for 9Ã—9 Go

A compact re-implementation of the **AlphaGo Zero** pipeline on a 9Ã—9 board.  
The engine couples **Monte-Carlo Tree Search (MCTS)** with twin neural networks (policy + value) and learns exclusively from **self-play**, needing **no human data**.

---

## âœ¨ Key Features

| Axis | Highlights |
|------|------------|
| **Search** | MCTS with **PUCT** selection, Dirichlet noise for exploration and value rollouts replaced by a NN evaluation :contentReference[oaicite:12]{index=12} |
| **Policy Net** | 8-block ResNet + Squeeze-and-Excitation, outputs full move distribution (81 + pass) in a single softmax :contentReference[oaicite:13]{index=13} |
| **Value Net** | 6 residual blocks with global attention, predicts win-probability in \[0, 1\] :contentReference[oaicite:14]{index=14} |
| **Dataset bootstrap** | 30 k 9Ã—9 games auto-annotated with **KataGo** (ELO â‰ˆ 14 k) â†’ >17 M positions after 8-fold symmetry augmentation :contentReference[oaicite:15]{index=15} |
| **Self-play loop** | 50 games â†’ 25 training epochs â†’ 20 evaluation games per iteration; automatic promotion when win-rate > 50 % :contentReference[oaicite:16]{index=16} |
| **Fast progression** | First 3 iterations jump from 60 % to 73 % win-rate vs previous nets before plateauing :contentReference[oaicite:17]{index=17} |
| **GUI** | Tkinter board (5Ã—5 â€“ 19Ã—19) for human play, AI vs AI or sandbox experimentation :contentReference[oaicite:18]{index=18} |

---

## ğŸ“‚ Project Structure

```

alpha\_go\_lite/
â”œâ”€â”€ MCTS\_GO.py                 # Core MCTS implementation
â”œâ”€â”€ go\_policy\_9x9\_v5.py        # Policy network definition & training helpers
â”œâ”€â”€ go\_value\_9x9\_v3.py         # Value network definition & training helpers
â”œâ”€â”€ katago\_relatives/
â”‚   â””â”€â”€ katago-opencl-linux/
â”‚       â””â”€â”€ networks/
â”‚           â””â”€â”€ reinforcement\_learning.py  # Self-play + RL driver
â”œâ”€â”€ gui/                        # Tkinter interface (human or AI play)
â””â”€â”€ requirements.txt

````

---

## âš™ï¸ Installation

```bash
git clone https://github.com/your-username/alpha_go_lite.git
cd alpha_go_lite

# Python â‰¥3.9 and virtual-env recommended
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt   # torch, numpy, matplotlib, tqdm, etc.
````

*KataGo binaries* (for optional data generation) are already vendored under `katago_relatives/`, but you can drop in a newer network if you wish.

---

## ğŸš€ Quick Start

### 1. Train from scratch

```bash
python katago_relatives/katago-opencl-linux/networks/reinforcement_learning.py \
  --iterations 100 \        # RL cycles
  --games 50 \              # self-play games per cycle
  --epochs 25 \             # NN epochs per cycle
  --mcts_sims 800 \         # simulations per move
  --eval_simulations 3200 \ # simulations for evaluation matches
  --eval_games 20           # eval matches vs previous net
```

Checkpoints are written every cycle; resume with `--resume checkpoints/latest.pth`.

### 2. Play against the AI

```bash
python gui/play.py --size 9 --mcts_sims 800
```

| Option                         | Purpose             |
| ------------------------------ | ------------------- |
| `--size {5,9,13,19}`           | Board size          |
| `--human_first`                | Let human start     |
| `--model checkpoints/best.pth` | Load custom network |

---

## ğŸ“Š Training Curves & Metrics

* **KL-divergence** of the policy drops from 3.3 â†’ 1.9 after 9 cycles, while **value MSE** reaches â‰ˆ 0.23.
* **Entropy monitoring** stays near 2.2 nats, balancing exploration and exploit confidence.
* Win-rate progression shown below (20 games / point): rapid gains, then convergence around the 50 % line as nets level up .

*(Add `docs/plots/*.png` once your training run finishes for visual curves.)*

---

## ğŸ”¬ Research Internals

* **Minimal input planes** â€“ only two binary channels (current / opponent stones) to force the net to infer tactics .
* **Top-k expansion** â€“ keep k best policy moves during node expansion to cap tree width.
* **RAdam + weight-decay** with periodic *optimizer refresh* every 50 cycles to escape plateaus, following the ablation in the report .

---

## ğŸ›£ Roadmap

* [ ] Add 13Ã—13 profile & curriculum resize
* [ ] Graphical analyzer (heat-map of MCTS visits)
* [ ] Export to SGF & GTP bridge for online play
* [ ] Distributed self-play (Ray / MPI)

---

## ğŸ¤ Contributing

1. Fork â†’ branch (`feat/xxx`)
2. `pre-commit run --all-files` (black & flake8)
3. PR with a clear description, screenshots if GUI-related.

---

## ğŸ“œ License

This project is released under the **MIT License**. See `LICENSE` for details.

---

## ğŸ™ Credits

* Inspired by DeepMindâ€™s AlphaGo Zero and the **KataGo** open-source community.
* Built by the PE 50 team (Ã‰cole Centrale de Lyon, 2025) .

Happy Go-coding! ğŸ¯â™Ÿï¸
